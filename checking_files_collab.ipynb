{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQlt2kEErCIe",
        "outputId": "a6334a23-87d8-4881-b8c9-d5abe7868949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: fastdtw in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy matplotlib tqdm fastdtw seaborn networkx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KUZeEUzefC3",
        "outputId": "6af3a994-11e9-45c8-cd31-dbf34f761f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "8mI0cZckguZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric torch-geometric-temporal"
      ],
      "metadata": {
        "id": "7W9l1UkBfFz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HD78cZtoiyG"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "from scipy.stats import pearsonr, zscore, rankdata\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric_temporal.nn.recurrent import GConvGRU, DCRNN\n",
        "from torch_geometric.utils import from_networkx, add_self_loops, to_undirected\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.data import Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7E4ETNiqCsM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y40lE1PeoiyN"
      },
      "outputs": [],
      "source": [
        "lines = []\n",
        "with open('/content/drive/MyDrive/EP1.01.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line)\n",
        "print(len(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZslQjahoiyP"
      },
      "outputs": [],
      "source": [
        "print(lines[0])\n",
        "print(lines[1])\n",
        "print(lines[2])\n",
        "print(lines[3])\n",
        "print(lines[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQjBQEXSoiyR"
      },
      "outputs": [],
      "source": [
        "split_ = lines[0].split()\n",
        "for i in range(len(split_)):\n",
        "    print(i, split_[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLZuG02toiyS"
      },
      "outputs": [],
      "source": [
        "event = []\n",
        "digit = []\n",
        "pos = []\n",
        "data = []\n",
        "line_counter = 0\n",
        "\n",
        "data_df = pd.DataFrame(columns=['event', 'digit', 'pos', 'data'])\n",
        "\n",
        "for line in tqdm(lines):\n",
        "    if line_counter<14_000_000:\n",
        "        split_line = line.split()\n",
        "        event.append(split_line[1])\n",
        "        digit.append(split_line[4])\n",
        "        pos.append(split_line[3])\n",
        "        data.append(split_line[6])\n",
        "        line_counter+=1\n",
        "\n",
        "data_df['event'] = event\n",
        "data_df['digit'] = digit\n",
        "data_df['pos'] = pos\n",
        "data_df['data'] = data\n",
        "\n",
        "num_events = data_df['event'].unique()\n",
        "print(len(num_events))\n",
        "\n",
        "del lines, event, digit, pos, data, num_events\n",
        "\n",
        "print(data_df.head())\n",
        "print(data_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALt_0MBXoiyT"
      },
      "outputs": [],
      "source": [
        "data_df['data'] = data_df['data'].apply(lambda x: [float(i) for i in x.split(',')])\n",
        "data_df['data'] = data_df['data'].apply(lambda x: x[:250])\n",
        "\n",
        "data_df['event'] = data_df['event'].apply(float)\n",
        "data_df['digit'] = data_df['digit'].apply(float)\n",
        "data_df['digit'] = data_df['digit'].apply(lambda x: 10 if x == -1 else x)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_df['data'] = data_df['data'].apply(lambda x: scaler.fit_transform(np.array(x).reshape(-1, 1)).flatten())\n",
        "\n",
        "# data_df.to_csv('data_df.csv')\n",
        "\n",
        "print(data_df.head(28))\n",
        "print(data_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JQjpDcIoiyT"
      },
      "outputs": [],
      "source": [
        "pos_values = data_df['pos'].unique()\n",
        "print(pos_values)\n",
        "data_lengths = data_df['data'].apply(len)\n",
        "unique_lengths = data_lengths.unique()\n",
        "print(unique_lengths)\n",
        "del data_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZRxbvyLoiyU"
      },
      "outputs": [],
      "source": [
        "def calculate_correlation(series1, series2):\n",
        "    return pearsonr(series1, series2)[0]\n",
        "\n",
        "def calculate_euclidean(series1, series2):\n",
        "    return euclidean(series1, series2)\n",
        "\n",
        "def calculate_dtw(series1, series2):\n",
        "    distance, path = fastdtw(series1, series2)\n",
        "    return distance\n",
        "\n",
        "probes = data_df['pos'].unique()\n",
        "similarity_matrix_corr = pd.DataFrame(index=probes, columns=probes)\n",
        "similarity_matrix_euc = pd.DataFrame(index=probes, columns=probes)\n",
        "similarity_matrix_dtw = pd.DataFrame(index=probes, columns=probes)\n",
        "\n",
        "\n",
        "for i, probe1 in enumerate(probes):\n",
        "    for j, probe2 in enumerate(probes):\n",
        "        if i < j:\n",
        "            series1 = data_df[data_df['pos'] == probe1]['data'].iloc[0]\n",
        "            series2 = data_df[data_df['pos'] == probe2]['data'].iloc[0]\n",
        "\n",
        "            corr = calculate_correlation(series1, series2)\n",
        "            euc = calculate_euclidean(series1, series2)\n",
        "            dtw = calculate_dtw(series1, series2)\n",
        "\n",
        "            similarity_matrix_corr.at[probe1, probe2] = corr\n",
        "            similarity_matrix_euc.at[probe2, probe1] = euc\n",
        "            similarity_matrix_dtw.at[probe1, probe2] = dtw\n",
        "\n",
        "np.fill_diagonal(similarity_matrix_corr.values, 0)\n",
        "np.fill_diagonal(similarity_matrix_euc.values, 0)\n",
        "np.fill_diagonal(similarity_matrix_dtw.values, 0)\n",
        "\n",
        "print(similarity_matrix_corr)\n",
        "print(similarity_matrix_euc)\n",
        "print(similarity_matrix_dtw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foaBYWRuoiyU"
      },
      "outputs": [],
      "source": [
        "def compare_sim_mat(similarity_matrix):\n",
        "    similarity_matrix = similarity_matrix.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    similarity_matrix = similarity_matrix.where(pd.notna(similarity_matrix), similarity_matrix.T)\n",
        "\n",
        "    range_val = similarity_matrix.max().max() - similarity_matrix.min().min()\n",
        "    if range_val == 0:\n",
        "        normalized_matrix = similarity_matrix\n",
        "    else:\n",
        "        normalized_matrix = (similarity_matrix - similarity_matrix.min().min()) / range_val\n",
        "\n",
        "    standardized_matrix = similarity_matrix.apply(lambda x: (x - x.mean()) / x.std(), axis=1)\n",
        "\n",
        "    ranked_matrix = similarity_matrix.rank(method='average')\n",
        "    ranked_matrix = (ranked_matrix - ranked_matrix.min().min()) / (ranked_matrix.max().max() - ranked_matrix.min().min())\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    sns.heatmap(normalized_matrix, annot=True, ax=axes[0])\n",
        "    axes[0].set_title('Normalized Similarity Matrix')\n",
        "\n",
        "    sns.heatmap(standardized_matrix, annot=True, ax=axes[1])\n",
        "    axes[1].set_title('Standardized Similarity Matrix')\n",
        "\n",
        "    sns.heatmap(ranked_matrix, annot=True, ax=axes[2])\n",
        "    axes[2].set_title('Ranked Similarity Matrix')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_sim_mat(similarity_matrix_corr)\n",
        "compare_sim_mat(similarity_matrix_euc)\n",
        "compare_sim_mat(similarity_matrix_dtw)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmS72pnzoiyV"
      },
      "outputs": [],
      "source": [
        "coordinates = {\n",
        "    'AF3': {'phi': (129.9+89.7)/2,\n",
        "            'theta': (52.2+41.0)/2},\n",
        "    'F7': {'phi': 137.2,\n",
        "            'theta': 83.1},\n",
        "    'F3': {'phi': 129.9,\n",
        "            'theta': 52.2},\n",
        "    'FC5': {'phi': (137.2+129.9+173.9+180)/4 ,\n",
        "            'theta': (83.1+52.2+95+45.2/4)},\n",
        "    'T7': {'phi': 173.9,\n",
        "            'theta': 95.0},\n",
        "    'P7': {'phi': 216.1,\n",
        "            'theta': 92.9},\n",
        "    'O1': {'phi': 250.6,\n",
        "            'theta': 89.2},\n",
        "    'O2': {'phi': 287.5,\n",
        "            'theta': 90.1},\n",
        "    'P8': {'phi': 322.7,\n",
        "            'theta': 94.9},\n",
        "    'T8': {'phi': 3.2,\n",
        "            'theta': 95.8},\n",
        "    'FC6': {'phi': (-1.0+3.2+40.3+49.8)/4,\n",
        "            'theta':(46.0+95.8+84.1+53.6)/4 },\n",
        "    'F4': {'phi': 49.8,\n",
        "            'theta': 53.6},\n",
        "    'F8': {'phi': 40.3,\n",
        "            'theta': 84.1},\n",
        "    'AF4': {'phi': (89.7+49.8)/2,\n",
        "            'theta': (41.0+53.6)/2}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u81GK3UoiyV"
      },
      "outputs": [],
      "source": [
        "def spherical_to_cartesian(phi, theta):\n",
        "    phi_rad = np.radians(phi)\n",
        "    theta_rad = np.radians(theta)\n",
        "    x = np.cos(phi_rad) * np.sin(theta_rad)\n",
        "    y = np.sin(phi_rad) * np.sin(theta_rad)\n",
        "    z = np.cos(theta_rad)\n",
        "    return x, y, z\n",
        "\n",
        "adjacency_matrix = np.zeros((len(coordinates), len(coordinates)))\n",
        "\n",
        "for i, (coord_i, values_i) in enumerate(coordinates.items()):\n",
        "    x_i, y_i, z_i = spherical_to_cartesian(values_i['phi'], values_i['theta'])\n",
        "    for j, (coord_j, values_j) in enumerate(coordinates.items()):\n",
        "        if i != j:\n",
        "            x_j, y_j, z_j = spherical_to_cartesian(values_j['phi'], values_j['theta'])\n",
        "            d = np.sqrt((x_j - x_i) ** 2 + (y_j - y_i) ** 2 + (z_j - z_i) ** 2)\n",
        "            w = 1 / np.sqrt(d)\n",
        "            adjacency_matrix[i, j] = w\n",
        "\n",
        "max_weight = adjacency_matrix.max()\n",
        "adjacency_matrix[adjacency_matrix > 0] /= max_weight\n",
        "\n",
        "np.fill_diagonal(adjacency_matrix, 1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "sns.heatmap(adjacency_matrix, annot=True, cmap='viridis', ax=ax)\n",
        "\n",
        "plt.title('EEG Electrodes Weighted Adjacency Matrix')\n",
        "plt.xlabel('Electrodes')\n",
        "plt.ylabel('Electrodes')\n",
        "plt.xticks(ticks=np.arange(len(coordinates)) + 0.5, labels=list(coordinates.keys()), rotation=45)\n",
        "plt.yticks(ticks=np.arange(len(coordinates)) + 0.5, labels=list(coordinates.keys()), rotation=45)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OldnBLWqoiyW"
      },
      "outputs": [],
      "source": [
        "def calculate_weights(coordinates):\n",
        "\n",
        "    num_nodes = len(coordinates)\n",
        "    adjacency_matrix = np.zeros((num_nodes, num_nodes))\n",
        "\n",
        "    for i, coord_i in enumerate(coordinates.keys()):\n",
        "        values_i = coordinates[coord_i]\n",
        "        x_i, y_i, z_i = spherical_to_cartesian(values_i['phi'], values_i['theta'])\n",
        "        for j, coord_j in enumerate(coordinates.keys()):\n",
        "            if i != j:\n",
        "                values_j = coordinates[coord_j]\n",
        "                x_j, y_j, z_j = spherical_to_cartesian(values_j['phi'], values_j['theta'])\n",
        "                d = np.sqrt((x_j - x_i) ** 2 + (y_j - y_i) ** 2 + (z_j - z_i) ** 2)\n",
        "                w = 1 / d\n",
        "                adjacency_matrix[i, j] = w\n",
        "\n",
        "    max_weight = adjacency_matrix.max()\n",
        "    adjacency_matrix[adjacency_matrix > 0] /= max_weight\n",
        "    np.fill_diagonal(adjacency_matrix, 1)\n",
        "\n",
        "    return adjacency_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9R2wz08oiyW"
      },
      "outputs": [],
      "source": [
        "class EEGGraphDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, edge_index, edge_weight):\n",
        "        self.dataframe = dataframe\n",
        "        self.edge_index = edge_index\n",
        "        self.edge_weight = edge_weight\n",
        "        self.edge_index = to_undirected(self.edge_index)\n",
        "        self.edge_index, _ = add_self_loops(self.edge_index)\n",
        "        self.unique_events = dataframe['event'].unique()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.unique_events)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        event_id = self.unique_events[idx]\n",
        "        event_data = self.dataframe[self.dataframe['event'] == event_id]\n",
        "        sequence = []\n",
        "        for time_step in range(len(event_data.iloc[0]['data'])):\n",
        "            x = torch.tensor([row.data[time_step] for row in event_data.itertuples(index=False)], dtype=torch.float).unsqueeze(1)\n",
        "            snapshot = Data(x=x, edge_index=self.edge_index, edge_attr=self.edge_weight)\n",
        "            sequence.append(snapshot)\n",
        "        label = event_data.iloc[0]['digit']\n",
        "        return sequence, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmssyy28oiyW"
      },
      "outputs": [],
      "source": [
        "def collate_graph_sequences(batch):\n",
        "    sequences, labels = zip(*batch)\n",
        "    batched_sequences = [Batch.from_data_list(sequence) for sequence in sequences]\n",
        "    return batched_sequences, torch.tensor(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()),\n",
        "                         key= lambda x: -x[1])[:10]:\n",
        "    print(\"{:>30}: {:>8}\".format(name, size))\n"
      ],
      "metadata": {
        "id": "Lh2TUIwFvwEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "def custom_train_test_split(dataset, test_size=0.2, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "    indices = np.arange(len(dataset))\n",
        "    print(indices.size)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    split_idx = int(len(dataset) * (1 - test_size))\n",
        "    print(split_idx)\n",
        "\n",
        "    train_indices, test_indices = indices[:split_idx], indices[split_idx:]\n",
        "\n",
        "    print('split done')\n",
        "\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    print(train_dataset)\n",
        "    test_dataset = Subset(dataset, test_indices)\n",
        "    print(test_dataset)\n",
        "\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "QSbwqHSHZPSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(adjacency_matrix))"
      ],
      "metadata": {
        "id": "wK9P54LkIJR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import networkx as nx\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "# def spherical_to_cartesian(phi, theta):\n",
        "#     phi_rad = np.radians(phi)\n",
        "#     theta_rad = np.radians(theta)\n",
        "#     x = np.cos(phi_rad) * np.sin(theta_rad)\n",
        "#     y = np.sin(phi_rad) * np.sin(theta_rad)\n",
        "#     z = np.cos(theta_rad)\n",
        "#     return x, y, z\n",
        "\n",
        "# def calculate_knn_graph(coordinates, k=5):\n",
        "#     positions = [spherical_to_cartesian(coord['phi'], coord['theta']) for coord in coordinates.values()]\n",
        "#     distance_matrix = squareform(pdist(positions))\n",
        "#     knn_graph = nx.Graph()\n",
        "\n",
        "#     for i, coord_i in enumerate(coordinates):\n",
        "#         distances = distance_matrix[i]\n",
        "#         nearest_indices = np.argsort(distances)[1:k+1]\n",
        "#         for j in nearest_indices:\n",
        "#             distance = distances[j]\n",
        "#             weight = 1 / distance\n",
        "#             knn_graph.add_edge(coord_i, list(coordinates.keys())[j], weight=weight)\n",
        "\n",
        "#     for coord in coordinates:\n",
        "#         knn_graph.add_edge(coord, coord, weight=1.0)\n",
        "\n",
        "#     return knn_graph\n",
        "\n",
        "# knn_graph = calculate_knn_graph(coordinates, k=5)\n",
        "\n",
        "# print(knn_graph)\n",
        "\n",
        "# edge_indices = []\n",
        "# edge_weights = []\n",
        "\n",
        "# for u, v, data in knn_graph.edges(data=True):\n",
        "#     if u != v:\n",
        "#         index_u = list(coordinates.keys()).index(u)\n",
        "#         index_v = list(coordinates.keys()).index(v)\n",
        "#         weight = data['weight']\n",
        "\n",
        "#         edge_indices.extend([[index_u, index_v], [index_v, index_u]])\n",
        "#         edge_weights.extend([weight, weight])\n",
        "\n",
        "# for i in range(len(coordinates)):\n",
        "#     edge_indices.append([i, i])\n",
        "#     edge_weights.append(1.0)\n",
        "\n",
        "# edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "# edge_weight = torch.tensor(edge_weights, dtype=torch.float)\n",
        "\n",
        "# print(edge_index.shape)\n",
        "# print(edge_weight.shape)\n",
        "\n",
        "# eeg_dataset = EEGGraphDataset(data_df, edge_index, edge_weight)\n",
        "\n",
        "# print(eeg_dataset[1])\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "\n",
        "# for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()),\n",
        "#                         key= lambda x: -x[1])[:10]:\n",
        "#     print(\"{:>30}: {:>8}\".format(name, size))\n",
        "\n",
        "# train_dataset, test_dataset = custom_train_test_split(eeg_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_graph_sequences)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_graph_sequences)\n"
      ],
      "metadata": {
        "id": "UkMFoMfJBEQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1trsZyKoiyX"
      },
      "outputs": [],
      "source": [
        "G = nx.complete_graph(14)\n",
        "edge_index = from_networkx(G).edge_index\n",
        "\n",
        "weights = calculate_weights(coordinates)\n",
        "edge_weights = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "eeg_dataset = EEGGraphDataset(data_df, edge_index, edge_weights)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()),\n",
        "                        key= lambda x: -x[1])[:10]:\n",
        "    print(\"{:>30}: {:>8}\".format(name, size))\n",
        "\n",
        "train_dataset, test_dataset = custom_train_test_split(eeg_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_graph_sequences)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_graph_sequences)\n",
        "\n",
        "del data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFGhAk2QoiyX"
      },
      "outputs": [],
      "source": [
        "class GRU(torch.nn.Module):\n",
        "    def __init__(self, node_features):\n",
        "        super(GRU, self).__init__()\n",
        "        self.recurrent1 = GConvGRU(node_features, 64, 2)\n",
        "        self.recurrent2 = GConvGRU(64, 32, 2)\n",
        "        self.recurrent3 = GConvGRU(32, 16, 2)\n",
        "        self.recurrent4 = GConvGRU(16, 8, 2)\n",
        "        self.linear = torch.nn.Linear(8, 11)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, batch):\n",
        "        h = self.recurrent1(x, edge_index, edge_weight)\n",
        "        h = F.relu(h)\n",
        "        h = self.recurrent2(h, edge_index, edge_weight)\n",
        "        h = F.relu(h)\n",
        "        h = self.recurrent3(h, edge_index, edge_weight)\n",
        "        h = F.relu(h)\n",
        "        h = self.recurrent4(h, edge_index, edge_weight)\n",
        "        h = F.relu(h)\n",
        "        h = self.linear(h)\n",
        "        graph_output = global_mean_pool(h, batch)\n",
        "        return graph_output\n",
        "\n",
        "class DC(torch.nn.Module):\n",
        "    def __init__(self, node_features):\n",
        "        super(DC, self).__init__()\n",
        "        self.recurrent1 = DCRNN(node_features, 32, 1)\n",
        "        self.recurrent2 = DCRNN(32, 16, 1)\n",
        "        self.recurrent3 = DCRNN(16, 16, 1)\n",
        "        self.linear = torch.nn.Linear(16, 11)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, batch):\n",
        "        h = self.recurrent1(x, edge_index, edge_weight)\n",
        "        h = F.relu(h)\n",
        "        h = self.recurrent2(h, edge_index, edge_weight)\n",
        "        h = F.relu(h)\n",
        "        h = self.recurrent3(h, edge_index, edge_weight)\n",
        "        h = F.relu(h)\n",
        "        h = self.linear(h)\n",
        "        graph_output = global_mean_pool(h, batch)\n",
        "        return graph_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jvSNV5DoiyY"
      },
      "outputs": [],
      "source": [
        "print(device)\n",
        "\n",
        "num_epochs = 100\n",
        "node_features = 1\n",
        "\n",
        "model = GRU(node_features).to(device)  # or DC(node_features)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=0.01)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "def calculate_accuracy(y_pred, y_true):\n",
        "    predicted = torch.argmax(y_pred, 1)\n",
        "    correct = (predicted == y_true).float().sum()\n",
        "    return (correct / y_true.shape[0]).item()\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    for batched_sequences, labels in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        sequence_outputs = []\n",
        "\n",
        "        for sequence in batched_sequences:\n",
        "          sequence.x = sequence.x.to(device)\n",
        "          sequence.edge_index = sequence.edge_index.to(device)\n",
        "          sequence.edge_weight = sequence.edge_attr.view(-1).to(device)\n",
        "          sequence.batch = sequence.batch.to(device)\n",
        "          graph_output = model(sequence.x, sequence.edge_index, sequence.edge_weight, sequence.batch)\n",
        "          sequence_outputs.append(graph_output)\n",
        "\n",
        "        sequence_outputs = torch.stack(sequence_outputs).mean(dim=0)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels.long()\n",
        "        loss = criterion(sequence_outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        total_correct += calculate_accuracy(sequence_outputs, labels)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(torch.argmax(sequence_outputs, 1).cpu().numpy())\n",
        "\n",
        "    train_accuracy = total_correct / len(train_loader)\n",
        "    train_confusion_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}, Train Accuracy: {train_accuracy}\")\n",
        "    print(\"Train Confusion Matrix:\\n\", train_confusion_matrix)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    total_correct = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batched_sequences, labels in test_loader:\n",
        "            sequence_outputs = []\n",
        "\n",
        "            for sequence in batched_sequences:\n",
        "              sequence.x = sequence.x.to(device)\n",
        "              sequence.edge_index = sequence.edge_index.to(device)\n",
        "              sequence.edge_weight = sequence.edge_attr.view(-1).to(device)\n",
        "              sequence.batch = sequence.batch.to(device)\n",
        "              graph_output = model(sequence.x, sequence.edge_index, sequence.edge_weight, sequence.batch)\n",
        "              sequence_outputs.append(graph_output)\n",
        "\n",
        "            sequence_outputs = torch.stack(sequence_outputs).mean(dim=0)\n",
        "            labels = labels.to(device)\n",
        "            labels = labels.long()\n",
        "            loss = criterion(sequence_outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            total_correct += calculate_accuracy(sequence_outputs, labels)\n",
        "            # all_labels.extend(labels.cpu().numpy())\n",
        "            # all_preds.extend(torch.argmax(sequence_outputs, 1).cpu().numpy())\n",
        "\n",
        "    test_accuracy = total_correct / len(test_loader)\n",
        "    # test_confusion_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    print(f\"Test Loss: {test_loss/len(test_loader)}, Test Accuracy: {test_accuracy}\")\n",
        "    # print(\"Test Confusion Matrix:\\n\", test_confusion_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1KAUdBBoiyb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}